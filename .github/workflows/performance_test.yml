# ğŸš¨ ========= COPILOTO/CURSOR: PIPELINE PERFORMANCE TESTING ========= ğŸš¨
#
# âš¡ PIPELINE DEDICADO DE PERFORMANCE:
#    âœ… Load testing automÃ¡tico
#    âœ… Benchmark de APIs
#    âœ… Monitoreo de mÃ©tricas de performance
#    âœ… AnÃ¡lisis de degradaciÃ³n
#    âœ… Alertas de performance crÃ­tica
#
# ğŸš€ TRIGGERS:
#    âœ… Schedule: Semanal y mensual
#    âœ… Manual: Para tests especÃ­ficos
#    âœ… Release: ValidaciÃ³n pre-producciÃ³n
#
# ğŸ“š REFERENCIA: /RULES.md secciÃ³n "ğŸ§  REGLAS PARA COPILOTO/CURSOR"
# 
# ================================================================

name: âš¡ Pipeline de Performance Testing

on:
  # â° Tests programados
  schedule:
    - cron: '0 3 * * 1'    # Weekly on Monday at 3 AM
    - cron: '0 1 1 * *'    # Monthly on 1st at 1 AM
  
  # ğŸš€ Tests en releases
  push:
    tags:
      - 'v*'
  
  # ğŸ”§ Trigger manual
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Tipo de test de performance'
        required: true
        default: 'standard'
        type: choice
        options:
        - standard
        - stress
        - endurance
        - spike
      concurrent_users:
        description: 'Usuarios concurrentes'
        required: false
        default: '100'
        type: string

env:
  PYTHON_VERSION: '3.11'
  TEST_DURATION: '300'  # 5 minutes
  
jobs:
  
  # ======================================
  # ğŸ—ï¸ SETUP Y PREPARACIÃ“N
  # ======================================
  setup_performance_env:
    name: ğŸ—ï¸ Setup Entorno Performance
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    outputs:
      app_url: ${{ steps.start_app.outputs.app_url }}
      
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: perf_test_pass
          POSTGRES_USER: perf_test_user
          POSTGRES_DB: perf_test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - name: ğŸ“¥ Checkout cÃ³digo
      uses: actions/checkout@v4
    
    - name: ğŸ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ğŸ“¦ Install Dependencies
      run: |
        cd backend
        python -m pip install --upgrade pip
        pip install -r requirements/base.txt
        pip install -r requirements/test.txt
        pip install uvicorn gunicorn
    
    - name: ğŸš€ Start Application
      id: start_app
      env:
        DATABASE_URL: postgresql://perf_test_user:perf_test_pass@localhost:5432/perf_test_db
        REDIS_URL: redis://localhost:6379/0
        TESTING: true
      run: |
        cd backend
        # Start app in background
        uvicorn app.main:app --host 0.0.0.0 --port 8000 &
        APP_PID=$!
        
        # Wait for app to be ready
        sleep 10
        
        # Health check
        curl -f http://localhost:8000/health || exit 1
        
        echo "app_url=http://localhost:8000" >> $GITHUB_OUTPUT
        echo "APP_PID=$APP_PID" >> $GITHUB_ENV
        
        # Keep running for other jobs
        wait

  # ======================================
  # ğŸ“Š BENCHMARK DE APIs
  # ======================================
  api_benchmark:
    name: ğŸ“Š API Benchmark
    runs-on: ubuntu-latest
    needs: setup_performance_env
    timeout-minutes: 20
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: perf_test_pass
          POSTGRES_USER: perf_test_user
          POSTGRES_DB: perf_test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - name: ğŸ“¥ Checkout cÃ³digo
      uses: actions/checkout@v4
    
    - name: ğŸ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: ğŸ“¦ Install Performance Tools
      run: |
        python -m pip install --upgrade pip
        pip install locust pytest-benchmark httpx
        
        # Install additional tools
        sudo apt-get update
        sudo apt-get install -y apache2-utils wrk
        
        # Install Node.js for additional tools
        curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
        sudo apt-get install -y nodejs
        npm install -g autocannon
    
    - name: ğŸš€ Start Test Application
      env:
        DATABASE_URL: postgresql://perf_test_user:perf_test_pass@localhost:5432/perf_test_db
        REDIS_URL: redis://localhost:6379/0
        TESTING: true
      run: |
        cd backend
        pip install -r requirements/base.txt
        uvicorn app.main:app --host 0.0.0.0 --port 8000 &
        sleep 10
        curl -f http://localhost:8000/health
    
    # âš¡ Apache Bench - Quick performance test
    - name: âš¡ Apache Bench Test
      run: |
        mkdir -p reports/performance
        echo "âš¡ Ejecutando Apache Bench..."
        
        # Health endpoint
        ab -n 1000 -c 10 -g reports/performance/ab_health.dat \
           http://localhost:8000/health > reports/performance/ab_health.txt
        
        # API endpoints (if available)
        if curl -s http://localhost:8000/api/v1/predict; then
          echo '{"data": [1,2,3,4,5]}' > test_payload.json
          ab -n 500 -c 5 -p test_payload.json -T application/json \
             http://localhost:8000/api/v1/predict > reports/performance/ab_predict.txt
        fi
    
    # ğŸ”¥ wrk - Advanced HTTP benchmarking
    - name: ğŸ”¥ wrk Benchmark
      run: |
        echo "ğŸ”¥ Ejecutando wrk benchmark..."
        
        # Basic performance test
        wrk -t4 -c12 -d30s --latency \
            http://localhost:8000/health > reports/performance/wrk_health.txt
        
        # Script for POST requests if predict endpoint exists
        if curl -s http://localhost:8000/api/v1/predict; then
          cat > predict_test.lua << 'EOF'
        wrk.method = "POST"
        wrk.body = '{"data": [1,2,3,4,5]}'
        wrk.headers["Content-Type"] = "application/json"
        EOF
          
          wrk -t2 -c8 -d30s --latency -s predict_test.lua \
              http://localhost:8000/api/v1/predict > reports/performance/wrk_predict.txt
        fi
    
    # ğŸš€ Autocannon - Node.js-based load testing
    - name: ğŸš€ Autocannon Test
      run: |
        echo "ğŸš€ Ejecutando autocannon..."
        
        # Health endpoint test
        autocannon -c 10 -d 30 -j \
          http://localhost:8000/health > reports/performance/autocannon_health.json
        
        # Pretty print results
        cat reports/performance/autocannon_health.json | \
          jq '{latency: .latency, requests: .requests, throughput: .throughput}' \
          > reports/performance/autocannon_summary.json
    
    - name: ğŸ“¤ Upload Benchmark Reports
      uses: actions/upload-artifact@v3
      with:
        name: api-benchmark-reports
        path: reports/performance/
        retention-days: 30

  # ======================================
  # ğŸ§ª LOAD TESTING CON LOCUST
  # ======================================
  load_testing:
    name: ğŸ§ª Load Testing
    runs-on: ubuntu-latest
    needs: setup_performance_env
    timeout-minutes: 25
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: perf_test_pass
          POSTGRES_USER: perf_test_user
          POSTGRES_DB: perf_test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - name: ğŸ“¥ Checkout cÃ³digo
      uses: actions/checkout@v4
    
    - name: ğŸ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: ğŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install locust
        cd backend
        pip install -r requirements/base.txt
    
    - name: ğŸš€ Start Application
      env:
        DATABASE_URL: postgresql://perf_test_user:perf_test_pass@localhost:5432/perf_test_db
        REDIS_URL: redis://localhost:6379/0
        TESTING: true
      run: |
        cd backend
        uvicorn app.main:app --host 0.0.0.0 --port 8000 &
        sleep 10
        curl -f http://localhost:8000/health
    
    - name: ğŸ“ Create Locust Test File
      run: |
        cat > locustfile.py << 'EOF'
        from locust import HttpUser, task, between
        import json
        import random
        
        class ApiUser(HttpUser):
            wait_time = between(1, 3)
            
            def on_start(self):
                # Setup user session
                pass
            
            @task(3)
            def health_check(self):
                self.client.get("/health")
            
            @task(1)
            def predict_endpoint(self):
                # Test predict endpoint if available
                payload = {
                    "data": [random.randint(1, 100) for _ in range(5)]
                }
                
                with self.client.post(
                    "/api/v1/predict",
                    json=payload,
                    catch_response=True
                ) as response:
                    if response.status_code == 404:
                        # Endpoint doesn't exist, mark as success to avoid failures
                        response.success()
                    elif response.status_code != 200:
                        response.failure(f"Got status code {response.status_code}")
        EOF
    
    - name: ğŸ§ª Run Load Test
      run: |
        mkdir -p reports/performance
        
        # Determine concurrent users
        USERS="${{ github.event.inputs.concurrent_users || '50' }}"
        
        echo "ğŸ§ª Ejecutando load test con $USERS usuarios..."
        
        # Run locust in headless mode
        locust -f locustfile.py --headless \
          --users=$USERS --spawn-rate=5 \
          --run-time=3m \
          --host=http://localhost:8000 \
          --html=reports/performance/locust_report.html \
          --csv=reports/performance/locust
    
    - name: ğŸ“Š Analyze Results
      run: |
        echo "ğŸ“Š Analizando resultados de performance..."
        
        # Create summary report
        cat > reports/performance/load_test_summary.md << 'EOF'
        # ğŸ§ª Load Test Results
        
        ## âš™ï¸ Test Configuration
        - ğŸ‘¥ **Users**: ${{ github.event.inputs.concurrent_users || '50' }}
        - â±ï¸ **Duration**: 3 minutes
        - ğŸ¯ **Target**: FastAPI Application
        
        ## ğŸ“Š Key Metrics
        - ğŸ“‹ Ver `locust_report.html` para mÃ©tricas detalladas
        - ğŸ“ˆ Ver `locust_stats.csv` para datos raw
        - ğŸ¯ Ver `locust_failures.csv` para errores
        
        ## ğŸš€ Performance Targets
        - ğŸ“Š **Response Time P95**: < 500ms
        - ğŸ¯ **Throughput**: > 100 RPS
        - ğŸš« **Error Rate**: < 1%
        EOF
    
    - name: ğŸ“¤ Upload Load Test Reports
      uses: actions/upload-artifact@v3
      with:
        name: load-test-reports
        path: reports/performance/
        retention-days: 60

  # ======================================
  # ğŸ“Š PERFORMANCE REPORT
  # ======================================
  performance_report:
    name: ğŸ“Š Performance Report
    runs-on: ubuntu-latest
    needs: [api_benchmark, load_testing]
    if: always()
    
    steps:
    - name: ğŸ“¥ Download Benchmark Reports
      uses: actions/download-artifact@v3
      with:
        name: api-benchmark-reports
        path: reports/benchmark/
    
    - name: ğŸ“¥ Download Load Test Reports
      uses: actions/download-artifact@v3
      with:
        name: load-test-reports
        path: reports/load-test/
    
    - name: ğŸ“Š Generate Performance Dashboard
      run: |
        mkdir -p reports/consolidated
        
        cat > reports/consolidated/performance_dashboard.md << 'EOF'
        # âš¡ Performance Testing Dashboard
        
        ## ğŸ“Š Resumen Ejecutivo
        - ğŸ• **Fecha**: $(date -u +"%Y-%m-%d %H:%M UTC")
        - ğŸ§ª **Load Testing**: Completado
        - ğŸ“Š **API Benchmark**: Ejecutado
        - ğŸ¯ **Tipo de Test**: ${{ github.event.inputs.test_type || 'Programado' }}
        
        ## ğŸ† MÃ©tricas Clave
        - âš¡ **Apache Bench**: Benchmark bÃ¡sico
        - ğŸ”¥ **wrk**: AnÃ¡lisis de latencia avanzado
        - ğŸš€ **Autocannon**: Throughput testing
        - ğŸ§ª **Locust**: Load testing completo
        
        ## ğŸ¯ Performance Targets
        - ğŸ“Š **Response Time P95**: < 500ms âš¡
        - ğŸ¯ **Throughput**: > 100 RPS ğŸš€
        - ğŸš« **Error Rate**: < 1% âœ…
        - ğŸ’¾ **Memory Usage**: < 512MB ğŸ“Š
        
        ## ğŸ“ˆ Tendencias
        - ğŸ“‹ Ver artifacts para reportes detallados
        - ğŸ” Comparar con tests anteriores
        - ğŸš¨ Alertas si degradaciÃ³n > 20%
        
        ## ğŸš€ Recomendaciones
        1. ğŸ” Revisar endpoints con mayor latencia
        2. ğŸ“Š Optimizar queries de base de datos
        3. ğŸš€ Implementar caching si es necesario
        4. ğŸ“ˆ Monitorear mÃ©tricas en producciÃ³n
        EOF
        
        # Replace date placeholder
        sed -i "s/\$(date -u +\"%Y-%m-%d %H:%M UTC\")/$(date -u +"%Y-%m-%d %H:%M UTC")/g" reports/consolidated/performance_dashboard.md
    
    - name: ğŸš¨ Performance Alert
      if: |
        github.event.inputs.test_type == 'critical' || 
        contains(github.event.head_commit.message, 'PERFORMANCE')
      run: |
        echo "ğŸš¨ ALERTA DE PERFORMANCE"
        echo "ğŸ“Š Revisar mÃ©tricas de performance inmediatamente"
        echo "ğŸ”— Reportes disponibles en artifacts"
    
    - name: ğŸ“¤ Upload Performance Dashboard
      uses: actions/upload-artifact@v3
      with:
        name: performance-dashboard
        path: reports/consolidated/
        retention-days: 90 