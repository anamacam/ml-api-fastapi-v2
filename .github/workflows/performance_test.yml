# 🚨 ========= COPILOTO/CURSOR: PIPELINE PERFORMANCE TESTING ========= 🚨
#
# ⚡ PIPELINE DEDICADO DE PERFORMANCE:
#    ✅ Load testing automático
#    ✅ Benchmark de APIs
#    ✅ Monitoreo de métricas de performance
#    ✅ Análisis de degradación
#    ✅ Alertas de performance crítica
#
# 🚀 TRIGGERS:
#    ✅ Schedule: Semanal y mensual
#    ✅ Manual: Para tests específicos
#    ✅ Release: Validación pre-producción
#
# 📚 REFERENCIA: /RULES.md sección "🧠 REGLAS PARA COPILOTO/CURSOR"
# 
# ================================================================

name: ⚡ Pipeline de Performance Testing

on:
  # ⏰ Tests programados
  schedule:
    - cron: '0 3 * * 1'    # Weekly on Monday at 3 AM
    - cron: '0 1 1 * *'    # Monthly on 1st at 1 AM
  
  # 🚀 Tests en releases
  push:
    tags:
      - 'v*'
  
  # 🔧 Trigger manual
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Tipo de test de performance'
        required: true
        default: 'standard'
        type: choice
        options:
        - standard
        - stress
        - endurance
        - spike
      concurrent_users:
        description: 'Usuarios concurrentes'
        required: false
        default: '100'
        type: string

env:
  PYTHON_VERSION: '3.11'
  TEST_DURATION: '300'  # 5 minutes
  
jobs:
  
  # ======================================
  # 🏗️ SETUP Y PREPARACIÓN
  # ======================================
  setup_performance_env:
    name: 🏗️ Setup Entorno Performance
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    outputs:
      app_url: ${{ steps.start_app.outputs.app_url }}
      
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: perf_test_pass
          POSTGRES_USER: perf_test_user
          POSTGRES_DB: perf_test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - name: 📥 Checkout código
      uses: actions/checkout@v4
    
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 📦 Install Dependencies
      run: |
        cd backend
        python -m pip install --upgrade pip
        pip install -r requirements/base.txt
        pip install -r requirements/test.txt
        pip install uvicorn gunicorn
    
    - name: 🚀 Start Application
      id: start_app
      env:
        DATABASE_URL: postgresql://perf_test_user:perf_test_pass@localhost:5432/perf_test_db
        REDIS_URL: redis://localhost:6379/0
        TESTING: true
      run: |
        cd backend
        # Start app in background
        uvicorn app.main:app --host 0.0.0.0 --port 8000 &
        APP_PID=$!
        
        # Wait for app to be ready
        sleep 10
        
        # Health check
        curl -f http://localhost:8000/health || exit 1
        
        echo "app_url=http://localhost:8000" >> $GITHUB_OUTPUT
        echo "APP_PID=$APP_PID" >> $GITHUB_ENV
        
        # Keep running for other jobs
        wait

  # ======================================
  # 📊 BENCHMARK DE APIs
  # ======================================
  api_benchmark:
    name: 📊 API Benchmark
    runs-on: ubuntu-latest
    needs: setup_performance_env
    timeout-minutes: 20
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: perf_test_pass
          POSTGRES_USER: perf_test_user
          POSTGRES_DB: perf_test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - name: 📥 Checkout código
      uses: actions/checkout@v4
    
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: 📦 Install Performance Tools
      run: |
        python -m pip install --upgrade pip
        pip install locust pytest-benchmark httpx
        
        # Install additional tools
        sudo apt-get update
        sudo apt-get install -y apache2-utils wrk
        
        # Install Node.js for additional tools
        curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
        sudo apt-get install -y nodejs
        npm install -g autocannon
    
    - name: 🚀 Start Test Application
      env:
        DATABASE_URL: postgresql://perf_test_user:perf_test_pass@localhost:5432/perf_test_db
        REDIS_URL: redis://localhost:6379/0
        TESTING: true
      run: |
        cd backend
        pip install -r requirements/base.txt
        uvicorn app.main:app --host 0.0.0.0 --port 8000 &
        sleep 10
        curl -f http://localhost:8000/health
    
    # ⚡ Apache Bench - Quick performance test
    - name: ⚡ Apache Bench Test
      run: |
        mkdir -p reports/performance
        echo "⚡ Ejecutando Apache Bench..."
        
        # Health endpoint
        ab -n 1000 -c 10 -g reports/performance/ab_health.dat \
           http://localhost:8000/health > reports/performance/ab_health.txt
        
        # API endpoints (if available)
        if curl -s http://localhost:8000/api/v1/predict; then
          echo '{"data": [1,2,3,4,5]}' > test_payload.json
          ab -n 500 -c 5 -p test_payload.json -T application/json \
             http://localhost:8000/api/v1/predict > reports/performance/ab_predict.txt
        fi
    
    # 🔥 wrk - Advanced HTTP benchmarking
    - name: 🔥 wrk Benchmark
      run: |
        echo "🔥 Ejecutando wrk benchmark..."
        
        # Basic performance test
        wrk -t4 -c12 -d30s --latency \
            http://localhost:8000/health > reports/performance/wrk_health.txt
        
        # Script for POST requests if predict endpoint exists
        if curl -s http://localhost:8000/api/v1/predict; then
          cat > predict_test.lua << 'EOF'
        wrk.method = "POST"
        wrk.body = '{"data": [1,2,3,4,5]}'
        wrk.headers["Content-Type"] = "application/json"
        EOF
          
          wrk -t2 -c8 -d30s --latency -s predict_test.lua \
              http://localhost:8000/api/v1/predict > reports/performance/wrk_predict.txt
        fi
    
    # 🚀 Autocannon - Node.js-based load testing
    - name: 🚀 Autocannon Test
      run: |
        echo "🚀 Ejecutando autocannon..."
        
        # Health endpoint test
        autocannon -c 10 -d 30 -j \
          http://localhost:8000/health > reports/performance/autocannon_health.json
        
        # Pretty print results
        cat reports/performance/autocannon_health.json | \
          jq '{latency: .latency, requests: .requests, throughput: .throughput}' \
          > reports/performance/autocannon_summary.json
    
    - name: 📤 Upload Benchmark Reports
      uses: actions/upload-artifact@v3
      with:
        name: api-benchmark-reports
        path: reports/performance/
        retention-days: 30

  # ======================================
  # 🧪 LOAD TESTING CON LOCUST
  # ======================================
  load_testing:
    name: 🧪 Load Testing
    runs-on: ubuntu-latest
    needs: setup_performance_env
    timeout-minutes: 25
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: perf_test_pass
          POSTGRES_USER: perf_test_user
          POSTGRES_DB: perf_test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - name: 📥 Checkout código
      uses: actions/checkout@v4
    
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: 📦 Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install locust
        cd backend
        pip install -r requirements/base.txt
    
    - name: 🚀 Start Application
      env:
        DATABASE_URL: postgresql://perf_test_user:perf_test_pass@localhost:5432/perf_test_db
        REDIS_URL: redis://localhost:6379/0
        TESTING: true
      run: |
        cd backend
        uvicorn app.main:app --host 0.0.0.0 --port 8000 &
        sleep 10
        curl -f http://localhost:8000/health
    
    - name: 📝 Create Locust Test File
      run: |
        cat > locustfile.py << 'EOF'
        from locust import HttpUser, task, between
        import json
        import random
        
        class ApiUser(HttpUser):
            wait_time = between(1, 3)
            
            def on_start(self):
                # Setup user session
                pass
            
            @task(3)
            def health_check(self):
                self.client.get("/health")
            
            @task(1)
            def predict_endpoint(self):
                # Test predict endpoint if available
                payload = {
                    "data": [random.randint(1, 100) for _ in range(5)]
                }
                
                with self.client.post(
                    "/api/v1/predict",
                    json=payload,
                    catch_response=True
                ) as response:
                    if response.status_code == 404:
                        # Endpoint doesn't exist, mark as success to avoid failures
                        response.success()
                    elif response.status_code != 200:
                        response.failure(f"Got status code {response.status_code}")
        EOF
    
    - name: 🧪 Run Load Test
      run: |
        mkdir -p reports/performance
        
        # Determine concurrent users
        USERS="${{ github.event.inputs.concurrent_users || '50' }}"
        
        echo "🧪 Ejecutando load test con $USERS usuarios..."
        
        # Run locust in headless mode
        locust -f locustfile.py --headless \
          --users=$USERS --spawn-rate=5 \
          --run-time=3m \
          --host=http://localhost:8000 \
          --html=reports/performance/locust_report.html \
          --csv=reports/performance/locust
    
    - name: 📊 Analyze Results
      run: |
        echo "📊 Analizando resultados de performance..."
        
        # Create summary report
        cat > reports/performance/load_test_summary.md << 'EOF'
        # 🧪 Load Test Results
        
        ## ⚙️ Test Configuration
        - 👥 **Users**: ${{ github.event.inputs.concurrent_users || '50' }}
        - ⏱️ **Duration**: 3 minutes
        - 🎯 **Target**: FastAPI Application
        
        ## 📊 Key Metrics
        - 📋 Ver `locust_report.html` para métricas detalladas
        - 📈 Ver `locust_stats.csv` para datos raw
        - 🎯 Ver `locust_failures.csv` para errores
        
        ## 🚀 Performance Targets
        - 📊 **Response Time P95**: < 500ms
        - 🎯 **Throughput**: > 100 RPS
        - 🚫 **Error Rate**: < 1%
        EOF
    
    - name: 📤 Upload Load Test Reports
      uses: actions/upload-artifact@v3
      with:
        name: load-test-reports
        path: reports/performance/
        retention-days: 60

  # ======================================
  # 📊 PERFORMANCE REPORT
  # ======================================
  performance_report:
    name: 📊 Performance Report
    runs-on: ubuntu-latest
    needs: [api_benchmark, load_testing]
    if: always()
    
    steps:
    - name: 📥 Download Benchmark Reports
      uses: actions/download-artifact@v3
      with:
        name: api-benchmark-reports
        path: reports/benchmark/
    
    - name: 📥 Download Load Test Reports
      uses: actions/download-artifact@v3
      with:
        name: load-test-reports
        path: reports/load-test/
    
    - name: 📊 Generate Performance Dashboard
      run: |
        mkdir -p reports/consolidated
        
        cat > reports/consolidated/performance_dashboard.md << 'EOF'
        # ⚡ Performance Testing Dashboard
        
        ## 📊 Resumen Ejecutivo
        - 🕐 **Fecha**: $(date -u +"%Y-%m-%d %H:%M UTC")
        - 🧪 **Load Testing**: Completado
        - 📊 **API Benchmark**: Ejecutado
        - 🎯 **Tipo de Test**: ${{ github.event.inputs.test_type || 'Programado' }}
        
        ## 🏆 Métricas Clave
        - ⚡ **Apache Bench**: Benchmark básico
        - 🔥 **wrk**: Análisis de latencia avanzado
        - 🚀 **Autocannon**: Throughput testing
        - 🧪 **Locust**: Load testing completo
        
        ## 🎯 Performance Targets
        - 📊 **Response Time P95**: < 500ms ⚡
        - 🎯 **Throughput**: > 100 RPS 🚀
        - 🚫 **Error Rate**: < 1% ✅
        - 💾 **Memory Usage**: < 512MB 📊
        
        ## 📈 Tendencias
        - 📋 Ver artifacts para reportes detallados
        - 🔍 Comparar con tests anteriores
        - 🚨 Alertas si degradación > 20%
        
        ## 🚀 Recomendaciones
        1. 🔍 Revisar endpoints con mayor latencia
        2. 📊 Optimizar queries de base de datos
        3. 🚀 Implementar caching si es necesario
        4. 📈 Monitorear métricas en producción
        EOF
        
        # Replace date placeholder
        sed -i "s/\$(date -u +\"%Y-%m-%d %H:%M UTC\")/$(date -u +"%Y-%m-%d %H:%M UTC")/g" reports/consolidated/performance_dashboard.md
    
    - name: 🚨 Performance Alert
      if: |
        github.event.inputs.test_type == 'critical' || 
        contains(github.event.head_commit.message, 'PERFORMANCE')
      run: |
        echo "🚨 ALERTA DE PERFORMANCE"
        echo "📊 Revisar métricas de performance inmediatamente"
        echo "🔗 Reportes disponibles en artifacts"
    
    - name: 📤 Upload Performance Dashboard
      uses: actions/upload-artifact@v3
      with:
        name: performance-dashboard
        path: reports/consolidated/
        retention-days: 90 